{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2496e838",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Data\n",
    "X = np.array([0.5, 2.5, 1.5])\n",
    "Y = np.array([0.2, 0.9, 0.6])\n",
    "\n",
    "# Parameters\n",
    "w, b = -2.0, -2.0\n",
    "lr = 0.3\n",
    "beta = 0.9     # decay rate for RMSProp\n",
    "eps = 1e-8\n",
    "iterations = 5\n",
    "\n",
    "# Accumulators for RMSProp\n",
    "vw, vb = 0.0, 0.0\n",
    "\n",
    "# Sigmoid\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "results = []\n",
    "\n",
    "for it in range(1, iterations + 1):\n",
    "    # Forward pass\n",
    "    z = w * X + b\n",
    "    y_pred = sigmoid(z)\n",
    "    y_pred = np.round(y_pred, 7)\n",
    "    \n",
    "    # Loss (MSE)\n",
    "    loss = np.mean((Y - y_pred) ** 2)\n",
    "    \n",
    "    # Gradients\n",
    "    grad_w = np.mean(2 * (y_pred - Y) * y_pred * (1 - y_pred) * X)\n",
    "    grad_b = np.mean(2 * (y_pred - Y) * y_pred * (1 - y_pred))\n",
    "    \n",
    "    # RMSProp accumulator update (exponential moving average)\n",
    "    vw = beta * vw + (1 - beta) * grad_w**2\n",
    "    vb = beta * vb + (1 - beta) * grad_b**2\n",
    "    \n",
    "    # RMSProp parameter update\n",
    "    w -= (lr / (np.sqrt(vw) + eps)) * grad_w\n",
    "    b -= (lr / (np.sqrt(vb) + eps)) * grad_b\n",
    "    \n",
    "    # Store iteration results\n",
    "    results.append({\n",
    "        \"iter\": it,\n",
    "        \"w\": w,\n",
    "        \"b\": b,\n",
    "        \"grad_w\": grad_w,\n",
    "        \"grad_b\": grad_b,\n",
    "        \"vw\": vw,\n",
    "        \"vb\": vb,\n",
    "        \"y_pred\": y_pred.tolist(),\n",
    "        \"loss\": loss\n",
    "    })\n",
    "\n",
    "# Convert results to DataFrame\n",
    "df = pd.DataFrame(results)\n",
    "\n",
    "import numpy as np\n",
    "# df = df.round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "07abd531",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>iter</th>\n",
       "      <th>w</th>\n",
       "      <th>b</th>\n",
       "      <th>grad_w</th>\n",
       "      <th>grad_b</th>\n",
       "      <th>vw</th>\n",
       "      <th>vb</th>\n",
       "      <th>y_pred</th>\n",
       "      <th>loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>-1.051321</td>\n",
       "      <td>-1.051321</td>\n",
       "      <td>-0.007606</td>\n",
       "      <td>-0.007770</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>[0.0474259, 0.0009111, 0.0066929]</td>\n",
       "      <td>0.394551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>-0.107661</td>\n",
       "      <td>-0.119116</td>\n",
       "      <td>-0.069839</td>\n",
       "      <td>-0.039034</td>\n",
       "      <td>0.000493</td>\n",
       "      <td>0.000158</td>\n",
       "      <td>[0.1712235, 0.0246115, 0.067339]</td>\n",
       "      <td>0.350287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.800100</td>\n",
       "      <td>0.701112</td>\n",
       "      <td>-0.219368</td>\n",
       "      <td>-0.064846</td>\n",
       "      <td>0.005256</td>\n",
       "      <td>0.000563</td>\n",
       "      <td>[0.4568709, 0.4041342, 0.4303049]</td>\n",
       "      <td>0.113554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.514959</td>\n",
       "      <td>-0.044786</td>\n",
       "      <td>0.068540</td>\n",
       "      <td>0.090536</td>\n",
       "      <td>0.005200</td>\n",
       "      <td>0.001326</td>\n",
       "      <td>[0.7504778, 0.937107, 0.8700343]</td>\n",
       "      <td>0.125774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.473410</td>\n",
       "      <td>-0.469416</td>\n",
       "      <td>0.009484</td>\n",
       "      <td>0.054679</td>\n",
       "      <td>0.004689</td>\n",
       "      <td>0.001492</td>\n",
       "      <td>[0.5529738, 0.7760181, 0.6742898]</td>\n",
       "      <td>0.048494</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   iter         w         b    grad_w    grad_b        vw        vb  \\\n",
       "0     1 -1.051321 -1.051321 -0.007606 -0.007770  0.000006  0.000006   \n",
       "1     2 -0.107661 -0.119116 -0.069839 -0.039034  0.000493  0.000158   \n",
       "2     3  0.800100  0.701112 -0.219368 -0.064846  0.005256  0.000563   \n",
       "3     4  0.514959 -0.044786  0.068540  0.090536  0.005200  0.001326   \n",
       "4     5  0.473410 -0.469416  0.009484  0.054679  0.004689  0.001492   \n",
       "\n",
       "                              y_pred      loss  \n",
       "0  [0.0474259, 0.0009111, 0.0066929]  0.394551  \n",
       "1   [0.1712235, 0.0246115, 0.067339]  0.350287  \n",
       "2  [0.4568709, 0.4041342, 0.4303049]  0.113554  \n",
       "3   [0.7504778, 0.937107, 0.8700343]  0.125774  \n",
       "4  [0.5529738, 0.7760181, 0.6742898]  0.048494  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9fbed3fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>iter</th>\n",
       "      <th>w</th>\n",
       "      <th>b</th>\n",
       "      <th>grad_w</th>\n",
       "      <th>grad_b</th>\n",
       "      <th>vw</th>\n",
       "      <th>vb</th>\n",
       "      <th>y_pred</th>\n",
       "      <th>loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>-1.700000</td>\n",
       "      <td>-1.700000</td>\n",
       "      <td>-0.007606</td>\n",
       "      <td>-0.007770</td>\n",
       "      <td>0.000058</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>[0.0474259, 0.0009111, 0.0066929]</td>\n",
       "      <td>0.394551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>-1.432955</td>\n",
       "      <td>-1.444204</td>\n",
       "      <td>-0.014859</td>\n",
       "      <td>-0.012681</td>\n",
       "      <td>0.000279</td>\n",
       "      <td>0.000221</td>\n",
       "      <td>[0.0724264, 0.0025991, 0.0140636]</td>\n",
       "      <td>0.388308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>-1.176351</td>\n",
       "      <td>-1.204411</td>\n",
       "      <td>-0.027560</td>\n",
       "      <td>-0.019782</td>\n",
       "      <td>0.001038</td>\n",
       "      <td>0.000613</td>\n",
       "      <td>[0.1033373, 0.0065184, 0.0267623]</td>\n",
       "      <td>0.378752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>-0.923347</td>\n",
       "      <td>-0.970580</td>\n",
       "      <td>-0.050567</td>\n",
       "      <td>-0.030792</td>\n",
       "      <td>0.003595</td>\n",
       "      <td>0.001561</td>\n",
       "      <td>[0.1427558, 0.0155919, 0.048849]</td>\n",
       "      <td>0.363074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>-0.672546</td>\n",
       "      <td>-0.738994</td>\n",
       "      <td>-0.091352</td>\n",
       "      <td>-0.047974</td>\n",
       "      <td>0.011940</td>\n",
       "      <td>0.003862</td>\n",
       "      <td>[0.1927479, 0.0363006, 0.0866217]</td>\n",
       "      <td>0.336529</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   iter         w         b    grad_w    grad_b        vw        vb  \\\n",
       "0     1 -1.700000 -1.700000 -0.007606 -0.007770  0.000058  0.000060   \n",
       "1     2 -1.432955 -1.444204 -0.014859 -0.012681  0.000279  0.000221   \n",
       "2     3 -1.176351 -1.204411 -0.027560 -0.019782  0.001038  0.000613   \n",
       "3     4 -0.923347 -0.970580 -0.050567 -0.030792  0.003595  0.001561   \n",
       "4     5 -0.672546 -0.738994 -0.091352 -0.047974  0.011940  0.003862   \n",
       "\n",
       "                              y_pred      loss  \n",
       "0  [0.0474259, 0.0009111, 0.0066929]  0.394551  \n",
       "1  [0.0724264, 0.0025991, 0.0140636]  0.388308  \n",
       "2  [0.1033373, 0.0065184, 0.0267623]  0.378752  \n",
       "3   [0.1427558, 0.0155919, 0.048849]  0.363074  \n",
       "4  [0.1927479, 0.0363006, 0.0866217]  0.336529  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Data\n",
    "X = np.array([0.5, 2.5, 1.5])\n",
    "Y = np.array([0.2, 0.9, 0.6])\n",
    "\n",
    "# Parameters\n",
    "w, b = -2.0, -2.0\n",
    "lr = 0.3\n",
    "eps = 1e-8\n",
    "iterations = 5\n",
    "\n",
    "# Accumulators for Adagrad\n",
    "vw, vb = 0.0, 0.0\n",
    "\n",
    "# Sigmoid\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "results = []\n",
    "\n",
    "for it in range(1, iterations+1):\n",
    "    # Forward pass\n",
    "    z = w * X + b\n",
    "    y_pred = sigmoid(z)\n",
    "    y_pred = np.round(y_pred, 7)\n",
    "    \n",
    "    # Loss (MSE)\n",
    "    loss = np.mean((Y - y_pred) ** 2)\n",
    "    \n",
    "    # Gradients\n",
    "    grad_w = np.mean(2 * (y_pred - Y) * y_pred * (1 - y_pred) * X)\n",
    "    grad_b = np.mean(2 * (y_pred - Y) * y_pred * (1 - y_pred))\n",
    "    \n",
    "    # Update accumulators\n",
    "    vw += grad_w**2\n",
    "    vb += grad_b**2\n",
    "    \n",
    "    # Adagrad update\n",
    "    w -= (lr / (np.sqrt(vw) + eps)) * grad_w\n",
    "    b -= (lr / (np.sqrt(vb) + eps)) * grad_b\n",
    "    \n",
    "    results.append({\n",
    "        \"iter\": it,\n",
    "        \"w\": w,\n",
    "        \"b\": b,\n",
    "        \"grad_w\": grad_w,\n",
    "        \"grad_b\": grad_b,\n",
    "        \"vw\": vw,\n",
    "        \"vb\": vb,\n",
    "        \"y_pred\": y_pred.tolist(),\n",
    "        \"loss\": loss\n",
    "    })\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(results)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f443310",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Training dataset\n",
    "X = np.array([0.5, 1.0])\n",
    "Y = np.array([0.2, 0.4])\n",
    "\n",
    "# Parameters\n",
    "w, b = 0.1, 0.0\n",
    "\n",
    "# Hyperparameters\n",
    "eta = 0.1    # learning rate\n",
    "epsilon = 1e-8\n",
    "beta1, beta2 = 0.9, 0.999\n",
    "iterations = 5\n",
    "\n",
    "# Adam moment estimates\n",
    "m_w, v_w = 0, 0\n",
    "m_b, v_b = 0, 0\n",
    "\n",
    "# Activation function\n",
    "def tanh(z):\n",
    "    return np.tanh(z)\n",
    "\n",
    "# Derivative of tanh\n",
    "def tanh_derivative(z):\n",
    "    return 1 - np.tanh(z) ** 2\n",
    "\n",
    "print(\"Initial w =\", w, \" b =\", b)\n",
    "print(\"===========================================\")\n",
    "\n",
    "for t in range(1, iterations + 1):\n",
    "    grad_w, grad_b, loss = 0, 0, 0\n",
    "    \n",
    "    # Loop over dataset\n",
    "    for x, y in zip(X, Y):\n",
    "        z = w * x + b\n",
    "        y_pred = tanh(z)\n",
    "        print(f\"x={x}, y={y}, z={z:.6f}, y_pred={y_pred:.6f}\")\n",
    "        \n",
    "        # Mean squared error loss\n",
    "        loss += 0.5 * (y_pred - y) ** 2\n",
    "        \n",
    "        # Gradients\n",
    "        dL_dy = (y_pred - y)\n",
    "        dy_dz = tanh_derivative(z)\n",
    "        dz_dw = x\n",
    "        dz_db = 1\n",
    "        \n",
    "        grad_w += dL_dy * dy_dz * dz_dw\n",
    "        grad_b += dL_dy * dy_dz * dz_db\n",
    "    \n",
    "    # Average gradients\n",
    "    grad_w /= len(X)\n",
    "    grad_b /= len(X)\n",
    "    loss /= len(X)\n",
    "\n",
    "    # Adam updates\n",
    "    m_w = beta1 * m_w + (1 - beta1) * grad_w\n",
    "    v_w = beta2 * v_w + (1 - beta2) * (grad_w ** 2)\n",
    "    m_b = beta1 * m_b + (1 - beta1) * grad_b\n",
    "    v_b = beta2 * v_b + (1 - beta2) * (grad_b ** 2)\n",
    "\n",
    "    # Bias correction\n",
    "    m_w_hat = m_w / (1 - beta1 ** t)\n",
    "    v_w_hat = v_w / (1 - beta2 ** t)\n",
    "    m_b_hat = m_b / (1 - beta1 ** t)\n",
    "    print(\"m_b_hat:\")\n",
    "    print(m_b_hat)\n",
    "    v_b_hat = v_b / (1 - beta2 ** t)\n",
    "    print(\"v_b_hat:\")\n",
    "    print(v_b_hat)\n",
    "\n",
    "    # Parameter updates\n",
    "    w = w - eta * m_w_hat / (np.sqrt(v_w_hat) + epsilon)\n",
    "    b = b - eta * m_b_hat / (np.sqrt(v_b_hat) + epsilon)\n",
    "\n",
    "    print(f\"Iteration {t}\")\n",
    "    print(f\" Loss={loss:.6f}\")\n",
    "    print(f\" grad_w={grad_w:.6f}, grad_b={grad_b:.6f}\")\n",
    "    print(f\" m_w={m_w:.6f}, v_w={v_w:.6f}, m_b={m_b:.6f}, v_b={v_b:.6f}\")\n",
    "    print(f\" w={w:.6f}, b={b:.6f}\")\n",
    "    print(\"===========================================\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a0cc007",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>iter</th>\n",
       "      <th>w</th>\n",
       "      <th>b</th>\n",
       "      <th>grad_w</th>\n",
       "      <th>grad_b</th>\n",
       "      <th>mw</th>\n",
       "      <th>mb</th>\n",
       "      <th>vw</th>\n",
       "      <th>vb</th>\n",
       "      <th>y_pred</th>\n",
       "      <th>loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1.200000</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.036644</td>\n",
       "      <td>0.056874</td>\n",
       "      <td>0.003664</td>\n",
       "      <td>0.005687</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>[0.8754466, 0.9929664, 0.9692311]</td>\n",
       "      <td>0.200401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.902129</td>\n",
       "      <td>0.600781</td>\n",
       "      <td>0.053188</td>\n",
       "      <td>0.075703</td>\n",
       "      <td>0.008617</td>\n",
       "      <td>0.012689</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>[0.8175745, 0.9801597, 0.9370267]</td>\n",
       "      <td>0.167137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.604746</td>\n",
       "      <td>0.301199</td>\n",
       "      <td>0.068509</td>\n",
       "      <td>0.090772</td>\n",
       "      <td>0.014606</td>\n",
       "      <td>0.020497</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>[0.7411292, 0.9456333, 0.8758794]</td>\n",
       "      <td>0.123671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.306247</td>\n",
       "      <td>0.000455</td>\n",
       "      <td>0.056027</td>\n",
       "      <td>0.084860</td>\n",
       "      <td>0.018748</td>\n",
       "      <td>0.026934</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>[0.6464731, 0.8597317, 0.7700012]</td>\n",
       "      <td>0.076620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.139693</td>\n",
       "      <td>-0.277714</td>\n",
       "      <td>-0.047362</td>\n",
       "      <td>0.026720</td>\n",
       "      <td>0.012137</td>\n",
       "      <td>0.026912</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>[0.5383194, 0.6826707, 0.6129728]</td>\n",
       "      <td>0.053953</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   iter         w         b    grad_w    grad_b        mw        mb        vw  \\\n",
       "0     1  1.200000  0.900000  0.036644  0.056874  0.003664  0.005687  0.000001   \n",
       "1     2  0.902129  0.600781  0.053188  0.075703  0.008617  0.012689  0.000004   \n",
       "2     3  0.604746  0.301199  0.068509  0.090772  0.014606  0.020497  0.000009   \n",
       "3     4  0.306247  0.000455  0.056027  0.084860  0.018748  0.026934  0.000012   \n",
       "4     5  0.139693 -0.277714 -0.047362  0.026720  0.012137  0.026912  0.000014   \n",
       "\n",
       "         vb                             y_pred      loss  \n",
       "0  0.000003  [0.8754466, 0.9929664, 0.9692311]  0.200401  \n",
       "1  0.000009  [0.8175745, 0.9801597, 0.9370267]  0.167137  \n",
       "2  0.000017  [0.7411292, 0.9456333, 0.8758794]  0.123671  \n",
       "3  0.000024  [0.6464731, 0.8597317, 0.7700012]  0.076620  \n",
       "4  0.000025  [0.5383194, 0.6826707, 0.6129728]  0.053953  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Data\n",
    "X = np.array([0.5, 2.5, 1.5])\n",
    "Y = np.array([0.2, 0.9, 0.6])\n",
    "\n",
    "# Parameters\n",
    "w, b = 1.5, 1.2\n",
    "lr = 0.3\n",
    "eps = 1e-8\n",
    "iterations = 5\n",
    "beta1, beta2 = 0.9, 0.999\n",
    "\n",
    "# Moment estimates for Adam\n",
    "mw = mb = 0.0\n",
    "vw = vb = 0.0\n",
    "\n",
    "# Sigmoid function\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "results = []\n",
    "\n",
    "for it in range(1, iterations+1):\n",
    "    # Forward pass\n",
    "    z = w * X + b\n",
    "    y_pred = sigmoid(z)\n",
    "    y_pred = np.round(y_pred, 7)\n",
    "    \n",
    "    # Loss (MSE)\n",
    "    loss = np.mean((Y - y_pred) ** 2)\n",
    "    \n",
    "    # Gradients\n",
    "    grad_w = np.mean(2 * (y_pred - Y) * y_pred * (1 - y_pred) * X)\n",
    "    grad_b = np.mean(2 * (y_pred - Y) * y_pred * (1 - y_pred))\n",
    "    \n",
    "    # Update biased first and second moments\n",
    "    mw = beta1 * mw + (1 - beta1) * grad_w\n",
    "    mb = beta1 * mb + (1 - beta1) * grad_b\n",
    "    vw = beta2 * vw + (1 - beta2) * (grad_w ** 2)\n",
    "    vb = beta2 * vb + (1 - beta2) * (grad_b ** 2)\n",
    "    \n",
    "    # Bias correction\n",
    "    mw_hat = mw / (1 - beta1 ** it)\n",
    "    mb_hat = mb / (1 - beta1 ** it)\n",
    "    vw_hat = vw / (1 - beta2 ** it)\n",
    "    vb_hat = vb / (1 - beta2 ** it)\n",
    "    \n",
    "    # Adam parameter update\n",
    "    w -= lr * mw_hat / (np.sqrt(vw_hat) + eps)\n",
    "    b -= lr * mb_hat / (np.sqrt(vb_hat) + eps)\n",
    "    \n",
    "    results.append({\n",
    "        \"iter\": it,\n",
    "        \"w\": w,\n",
    "        \"b\": b,\n",
    "        \"grad_w\": grad_w,\n",
    "        \"grad_b\": grad_b,\n",
    "        \"mw\": mw,\n",
    "        \"mb\": mb,\n",
    "        \"vw\": vw,\n",
    "        \"vb\": vb,\n",
    "        \"y_pred\": y_pred.tolist(),\n",
    "        \"loss\": loss\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302beb69",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
