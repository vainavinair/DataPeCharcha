{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05451184",
   "metadata": {},
   "source": [
    "## 1. Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "015bd39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "# --- Step 1: Synthetic Data ---\n",
    "x = torch.linspace(-5, 5, 100).unsqueeze(1)\n",
    "y = 3 * x + 0.8 * torch.randn(x.size())\n",
    "\n",
    "# --- Step 2: Define two different models ---\n",
    "model_A = nn.Sequential(nn.Linear(1, 1))                     \n",
    "model_B = nn.Sequential(nn.Linear(1, 10), nn.ReLU(), nn.Linear(10, 1))  \n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer_A = optim.SGD(model_A.parameters(), lr=0.01)\n",
    "optimizer_B = optim.Adam(model_B.parameters(), lr=0.001)\n",
    "\n",
    "writer_A = SummaryWriter(log_dir=\"runs/model_A\")\n",
    "writer_B = SummaryWriter(log_dir=\"runs/model_B\")\n",
    "\n",
    "epochs = 100\n",
    "\n",
    "# --- Step 3: Training loop ---\n",
    "for epoch in range(epochs):\n",
    "    # ===== Model A =====\n",
    "    optimizer_A.zero_grad()\n",
    "    outputs_A = model_A(x)\n",
    "    loss_A = criterion(outputs_A, y)\n",
    "    loss_A.backward()\n",
    "    optimizer_A.step()\n",
    "\n",
    "    # Log scalar metrics\n",
    "    writer_A.add_scalar(\"Loss/train\", loss_A.item(), epoch)\n",
    "    writer_A.add_scalar(\"LR\", optimizer_A.param_groups[0]['lr'], epoch)\n",
    "\n",
    "    # Log histograms for weights, biases, and gradients\n",
    "    for name, param in model_A.named_parameters():\n",
    "        writer_A.add_histogram(f\"{name}/weights\", param, epoch)\n",
    "        if param.grad is not None:\n",
    "            writer_A.add_histogram(f\"{name}/grads\", param.grad, epoch)\n",
    "\n",
    "    # ===== Model B =====\n",
    "    optimizer_B.zero_grad()\n",
    "    outputs_B = model_B(x)\n",
    "    loss_B = criterion(outputs_B, y)\n",
    "    loss_B.backward()\n",
    "    optimizer_B.step()\n",
    "\n",
    "    writer_B.add_scalar(\"Loss/train\", loss_B.item(), epoch)\n",
    "    writer_B.add_scalar(\"LR\", optimizer_B.param_groups[0]['lr'], epoch)\n",
    "\n",
    "    for name, param in model_B.named_parameters():\n",
    "        writer_B.add_histogram(f\"{name}/weights\", param, epoch)\n",
    "        if param.grad is not None:\n",
    "            writer_B.add_histogram(f\"{name}/grads\", param.grad, epoch)\n",
    "\n",
    "    # --- Combined loss (for easy comparison in one chart) ---\n",
    "    writer_A.add_scalars(\"Comparison/Loss\", {\"Model_A\": loss_A.item(), \"Model_B\": loss_B.item()}, epoch)\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch:03d}: Loss A = {loss_A.item():.4f}, Loss B = {loss_B.item():.4f}\")\n",
    "\n",
    "# --- Step 4: Visualize model predictions ---\n",
    "with torch.no_grad():\n",
    "    pred_A = model_A(x)\n",
    "    pred_B = model_B(x)\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.scatter(x.numpy(), y.numpy(), label=\"True Data\", color=\"gray\")\n",
    "plt.plot(x.numpy(), pred_A.numpy(), label=\"Model A (Linear)\", color=\"blue\")\n",
    "plt.plot(x.numpy(), pred_B.numpy(), label=\"Model B (Deep)\", color=\"red\")\n",
    "plt.legend()\n",
    "plt.title(\"Model Predictions\")\n",
    "plt.savefig(\"predictions.png\")\n",
    "\n",
    "# Log the comparison plot as image\n",
    "image = np.moveaxis(plt.imread(\"predictions.png\"), -1, 0)\n",
    "writer_A.add_image(\"Predictions\", image)\n",
    "writer_B.add_image(\"Predictions\", image)\n",
    "\n",
    "# --- Step 5: Flush and close writers ---\n",
    "writer_A.flush()\n",
    "writer_B.flush()\n",
    "writer_A.close()\n",
    "writer_B.close()\n",
    "time.sleep(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e75428",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Install wandb if not installed ---\n",
    "# pip install wandb torch matplotlib\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import wandb\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "\n",
    "# --- Step 0: Set W&B to offline mode ---\n",
    "os.environ[\"WANDB_MODE\"] = \"offline\"  # ensures everything stays local\n",
    "\n",
    "# --- Step 1: Synthetic Data ---\n",
    "x = torch.linspace(-5, 5, 100).unsqueeze(1)\n",
    "y = 3 * x + 0.8 * torch.randn(x.size())\n",
    "\n",
    "# --- Step 2: Define two models ---\n",
    "model_A = nn.Sequential(nn.Linear(1, 1))                     # simple linear\n",
    "model_B = nn.Sequential(nn.Linear(1, 10), nn.ReLU(), nn.Linear(10, 1))  # slightly deeper\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer_A = optim.SGD(model_A.parameters(), lr=0.01)\n",
    "optimizer_B = optim.Adam(model_B.parameters(), lr=0.01)\n",
    "\n",
    "# --- Step 3: Initialize W&B offline project ---\n",
    "wandb.init(project=\"local-two-models\", name=\"Combined_Run\", reinit=True)\n",
    "\n",
    "# Optional: Watch models to log weights and gradients\n",
    "wandb.watch(model_A, log=\"all\", log_freq=10)\n",
    "wandb.watch(model_B, log=\"all\", log_freq=10)\n",
    "\n",
    "epochs = 100\n",
    "\n",
    "# --- Step 4: Training Loop ---\n",
    "for epoch in range(epochs):\n",
    "    # ---- Train Model A ----\n",
    "    optimizer_A.zero_grad()\n",
    "    out_A = model_A(x)\n",
    "    loss_A = criterion(out_A, y)\n",
    "    loss_A.backward()\n",
    "    optimizer_A.step()\n",
    "\n",
    "    # ---- Train Model B ----\n",
    "    optimizer_B.zero_grad()\n",
    "    out_B = model_B(x)\n",
    "    loss_B = criterion(out_B, y)\n",
    "    loss_B.backward()\n",
    "    optimizer_B.step()\n",
    "\n",
    "    # ---- Log metrics for both models ----\n",
    "    wandb.log({\n",
    "        \"epoch\": epoch,\n",
    "        \"Loss/Model_A\": loss_A.item(),\n",
    "        \"Loss/Model_B\": loss_B.item()\n",
    "    })\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch:03d} | Loss A: {loss_A.item():.4f} | Loss B: {loss_B.item():.4f}\")\n",
    "\n",
    "# --- Step 5: Log prediction plots ---\n",
    "with torch.no_grad():\n",
    "    pred_A = model_A(x).detach().numpy()\n",
    "    pred_B = model_B(x).detach().numpy()\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.scatter(x.numpy(), y.numpy(), label=\"True Data\", color=\"gray\")\n",
    "plt.plot(x.numpy(), pred_A, label=\"Model A Prediction\", color=\"blue\")\n",
    "plt.plot(x.numpy(), pred_B, label=\"Model B Prediction\", color=\"red\")\n",
    "plt.legend()\n",
    "plt.title(\"Model Predictions\")\n",
    "plt.savefig(\"predictions.png\")\n",
    "plt.close()\n",
    "\n",
    "# Log the plot to W&B\n",
    "wandb.log({\"Predictions\": wandb.Image(\"predictions.png\")})\n",
    "\n",
    "# --- Step 6: Finish W&B run ---\n",
    "wandb.finish()\n",
    "time.sleep(1)\n",
    "\n",
    "print(\"✅ Training complete! You can view your local W&B dashboard with `wandb local` and open http://localhost:8080\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e229ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "import mlflow\n",
    "import mlflow.pytorch\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# --- Windows-friendly paths ---\n",
    "MLFLOW_RUNS_PATH = \"D:\\\\The-Office\\\\Substack\\\\experiment-tracking\"  # change if needed\n",
    "os.makedirs(MLFLOW_RUNS_PATH, exist_ok=True)\n",
    "mlflow.set_tracking_uri(f\"file:///{MLFLOW_RUNS_PATH}\")\n",
    "mlflow.set_experiment(\"Two_Model_Comparison\")\n",
    "\n",
    "# --- Synthetic Data ---\n",
    "x = torch.linspace(-5, 5, 100).unsqueeze(1)\n",
    "y = 3 * x + 0.8 * torch.randn(x.size())\n",
    "\n",
    "# --- Models ---\n",
    "model_A = nn.Sequential(nn.Linear(1, 1))\n",
    "model_B = nn.Sequential(nn.Linear(1, 10), nn.ReLU(), nn.Linear(10, 1))\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer_A = optim.SGD(model_A.parameters(), lr=0.01)\n",
    "optimizer_B = optim.Adam(model_B.parameters(), lr=0.01)\n",
    "\n",
    "epochs = 100\n",
    "\n",
    "# --- Train & log model function ---\n",
    "def train_model(model, optimizer, model_name):\n",
    "    with mlflow.start_run(run_name=model_name):\n",
    "        # Log hyperparameters\n",
    "        mlflow.log_param(\"model_name\", model_name)\n",
    "        mlflow.log_param(\"optimizer\", optimizer.__class__.__name__)\n",
    "        mlflow.log_param(\"learning_rate\", optimizer.param_groups[0]['lr'])\n",
    "        mlflow.log_param(\"epochs\", epochs)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(x)\n",
    "            loss = criterion(outputs, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Log loss per epoch\n",
    "            mlflow.log_metric(\"train_loss\", loss.item(), step=epoch)\n",
    "\n",
    "            if epoch % 10 == 0:\n",
    "                print(f\"{model_name} | Epoch {epoch:03d} | Loss: {loss.item():.4f}\")\n",
    "\n",
    "        # --- Log model with input example (convert tensor to numpy) ---\n",
    "        mlflow.pytorch.log_model(model, name=\"model\", input_example=x[:5].numpy())\n",
    "\n",
    "        # --- Log prediction plot ---\n",
    "        with torch.no_grad():\n",
    "            preds = model(x).detach().numpy()\n",
    "        plt.figure(figsize=(6,4))\n",
    "        plt.scatter(x.numpy(), y.numpy(), label=\"True Data\", color=\"gray\")\n",
    "        plt.plot(x.numpy(), preds, label=f\"{model_name} Prediction\", color=\"orange\")\n",
    "        plt.legend()\n",
    "        plt.title(f\"{model_name} Predictions\")\n",
    "        plot_path = f\"{model_name}_predictions.png\"\n",
    "        plt.savefig(plot_path)\n",
    "        plt.close()\n",
    "        mlflow.log_artifact(plot_path)\n",
    "\n",
    "# --- Train both models ---\n",
    "train_model(model_A, optimizer_A, \"Model_A\")\n",
    "train_model(model_B, optimizer_B, \"Model_B\")\n",
    "\n",
    "print(\"\\n✅ Training complete! Start MLflow UI:\")\n",
    "print(f\"python -m mlflow ui --backend-store-uri file:///{MLFLOW_RUNS_PATH}\")\n",
    "print(\"Open http://127.0.0.1:5000 in your browser.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a696411a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "substack",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
